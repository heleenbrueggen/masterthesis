\documentclass[10pt, a4paper, titlepage]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx, latexsym}
\usepackage{titling}
\setlength{\droptitle}{-25em}
\renewcommand{\maketitlehooka}{\Large}
\usepackage{setspace}
\usepackage{amssymb, amsmath, amsthm}
\usepackage[export]{adjustbox}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{epstopdf}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\usepackage{titling}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{colortbl}
\usepackage{lscape}
\usepackage[sort&compress,square,comma,authoryear]{natbib}
\hypersetup{
    pdftitle={Research Report Heleen Br端ggen},
    pdfauthor={Heleen Br端ggen},
    pdfsubject={Research Report Heleen Br端ggen},
    pdfkeywords={},
    bookmarksnumbered=true,
    bookmarksopen=true,
    bookmarksopenlevel=1,
    colorlinks=false,
    pdfstartview=Fit,
    pdfpagemode=UseNone
}

\singlespacing

\begin{document}
\begin{titlingpage}
\begin{center}
\Huge\textbf{Master Research Report:  \\ Multilevel Multivariate Imputation by Chained Equations through Bayesian Additive Regression Trees} \\
\Large\textit{Methodology and Statistics for the Behavioural, Biomedical and Social Sciences}

\vspace{.5cm}

\normalsize\textit{Heleen Br端ggen}

\vspace{15cm}

\begin{minipage}{0.5\textwidth}
\begin{flushleft}

\textbf{Word count:} \\
\textbf{Candidate Journal:} \\
\textbf{FETC Case Number:} \\
\textbf{Supervisors:} \\
MSc. T. Volker \\
Dr. G. Vink \\
 MSc. H. Oberman
\end{flushleft}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\begin{flushright}

2524 \\
Computational Statistics \& Data Analysis \\
23-1778 \\
------------------------\\
Utrecht University \\
Utrecht University \\
Utrecht University
\end{flushright}
\end{minipage}

\end{center}
\end{titlingpage}

\newpage

\section{Introduction}

Incomplete data is a common challenge in many fields of research. A frequent approach for dealing with incomplete data is listwise deletion, also known as complete-case analysis, which is to remove all incomplete cases from the data. However, this could possibly lead to biased results if the data is not Missing Completely At Random (MCAR), meaning the cause of the missing data is unrelated to the data \citep{buurenFlexibleImputationMissing2018, kang2013, enders2017, austin2021, rubin1976}. Furthermore, other approaches to dealing with incomplete data include: pairwise deletion, mean imputation and regression imputation, which also yield biased results \citep{buurenFlexibleImputationMissing2018}. Pairwise deletion, also known as available-case analysis, is to remove all incomplete cases from the analysis when considering a specific pair of variables. Pairwise deletion leads to unbiased results when correlation between variables are low and the data is MCAR. Mean imputation is to replace missing values with the mean of the observed values. Mean imputation will bias almost all estimates except the mean when the data is not MCAR. Regression imputation is to replace missing values with the predicted values from a regression model and is unbiased when the data is Missing At Random (MAR), meaning that the missing data is related to the observed data \citep{rubin1976}, and the factor influencing the missingness is present in the data \citep{buurenFlexibleImputationMissing2018}. So, in order to use these ad hoc strategies to deal with incomplete data correctly, the missing data mechanism should be carefully determined. However, determining the missing data mechanism is often difficult and MCAR is in practise an unrealistic assumption \citep{buurenFlexibleImputationMissing2018}. We can imagine that when a system becomes more complex, thus increases the amount of parameters, testing these assumptions also increases in complexity.

One of these complex systems are multilevel data structures. Multilevel data is hierarchically structured, where, for example, students are nested within schools, or patients are nested within hospitals \citep{hox2017, hox2011}. Thus, in these types of data sets there are level-1 and level-2 variables. Level-1 variables relate to the individual within a class and level-2 variables relate to the class as a whole. The recommended statistical technique to analyzing these models are multilevel models as it accounts for the specific dependencies in the  multilevel data sets \citep{hox2017, hox2011, ludtke2017}. It can contain both level-1 and level-2 variables, random intercepts, random slopes, and cross-level interactions \citep{hox2017, hox2011}.

Multiple imputation (MI) is considered a valid method for dealing with incomplete data and allows us the separate the missing data problem from the analysis problem \citep{mistlerComparisonJointModel2017, buurenFlexibleImputationMissing2018, enders2017, burgette2010, austin2021, audigier2018, vanbuuren2007, grund2021, hughes2014}. MI imputes each missing value in the data set more than once given the observed data, thereby considering necessary variation associated with the missingness problem. The multiply imputed data sets are analyzed, and the corresponding inferences are pooled according to Rubin's rules \citep{buurenFlexibleImputationMissing2018, austin2021, rubin1987, carpenter2013}.
Generally, multiple imputation operates under two frameworks: joint modeling (JM) and fully conditional specification (FCS) \citep{mistlerComparisonJointModel2017, buurenFlexibleImputationMissing2018, enders2018, enders2018a, hughes2014}. JM employs a multivariate data distribution and regresses incomplete variables on all complete variables to impute missing values. FCS, or chained equations, iteratively imputes one variable with missing values at a time through conditional univariate distributions regressing an incomplete variable on complete variables and previously imputed variables \citep{mistlerComparisonJointModel2017, buurenFlexibleImputationMissing2018, enders2016, enders2018, enders2018a, hughes2014, grund2018a}.

JM and FCS use multivariate and univariate linear mixed models respectively in a multilevel context \citep{mistlerComparisonJointModel2017, enders2018}. The implementation of JM and FCS in a multilevel context are equivalent in a two-level random intercept context with normally distributed data but differ beyond that \citep{enders2016, enders2018a}. Overall, FCS is believed to be more flexible than JM \citep{audigier2018, burgette2010, vanbuuren2007, grund2018a} and, thus, may be better suited for multilevel data. In FCS, one needs to define conditional models for all variables with missing values \citep{mistlerComparisonJointModel2017, buurenFlexibleImputationMissing2018, enders2018, enders2018a, enders2016, hughes2014, grund2018a} and the imputation models should at least be as general as the analysis model and preferably all-encompassing \citep{grund2018}. However, the complexity of the multilevel analysis model is built step-wise with non-linearities \citep{hox2017, hox2011}. Thus, defining imputation models for a multilevel data set is quite challenging \citep{buurenFlexibleImputationMissing2018, burgette2010, hox2011}.

% There are three ad-hoc strategies for dealing with multilevel missing data \citep{buurenFlexibleImputationMissing2018}. The first is listwise deletion, which will also lead to bias results if the data is not MCAR in a multilevel context \cite{grund2016, buurenFlexibleImputationMissing2018, ludtke2017}. Second is ignoring the multilevel structure and imputing the data as if it were a single-level data set. However, this will underestimate the Intra-Class Correlation (ICC) \citep{buurenFlexibleImputationMissing2018, ludtke2017, taljaard2008, hox2011}. The ICC can be interpreted as the expected correlation between two randomly sampled individuals from the same group or the proportion of the total variance at level-2 \citep{gulliford2005, shieh2012, hox2011}. The third strategy is fixed effect imputation: adding group dummy variables representing the group effects \citep{ludtke2017, enders2016}. Again, estimates in this method can be biased: the ICC will be overestimated and the between cluster regression coefficients will be underestimated \citep{ludtke2017, buurenFlexibleImputationMissing2018}.

Using non-parametric tree-based models as imputation models might solve this problem. Tree-based models use recursive partitioning to split the data into smaller subgroups based on the predictor variables maximizing the homogeneity of the subgroups. Tree-based models are non-parametric, which means that they do not assume a specific distribution of the data. Thus, they can handle non-linear relationships and interactions between the predictor variables well. Furthermore they handle continuous and catergorical variables simultaneously \citep{hill2020, burgette2010, lin2019, chipman2010, james2021, salditt2023, breiman1984}.

In a single-level imputation context, the use of tree-based, non-parametric models like regression trees, random forests or Bayesian Additive Regression Trees (BART) simplified imputation models and performed better than parametric methods: the imputations showed better confidence interval coverage of the population parameters, lower variance and lower bias, especially in non-linear and interactive contexts \citep{burgette2010, xu2016, silva2022}. Others have also found lower normalized root mean squared error (NRMSE), which in essence encapsulates the bias of the imputations, when imputing with a random forest algorithm compared to MICE and KNN imputation \citep{stekhoven2012, waljee2013}. Furthermore, they also found that the algorithm reduced computational time and could handle multivariate data consisting of both continuous and categorical data simultaneously.

BART models have also been implemented in a multilevel prediction context. However, multilevel-BART models (M-BART) have predominantly been implemented with random intercepts and no random slopes and cross-level interactions \citep{chen2020, wagner2020, tan2016, wundervald2022}. \citet{wagner2020} have found that this random intercept M-BART model provided better predictions with a lower Mean Squared Error (MSE) compared to a parametric multilevel model, \citet{tan2016} found higher Area Under the Curve values, and \citet{chen2020} found better predictions and better coverage compared to parametric models and a single-level BART model. Other researchers modeled the random intercept as an extra split on each terminal node within the BART algorithm and found a lower MSE compared to a standard BART model and parametric multilevel models \citep{wundervald2022}. \citet{dorie2022} developed a multilevel BART model that included random intercepts and random slopes by combining BART with the Stan algorithm. However, the random intercept and slope are modeled by Stan, which is a parametric method. Their results showed that their algorithm `stan4bart` showed better coverage of the population value and lower Root Mean Squared Error (RMSE) compared to BART models with varying intercept, BART models ignoring the multilevel structure, Bayesian Causal Forests (BCF), and parametric multilevel models.

In spite of these promising findings: tree-based model performing well in single-level imputation context \citep{burgette2010, xu2016, silva2022, stekhoven2012, waljee2013} and M-BART models performing well in a multilevel prediction context \citep{chen2020, wagner2020, tan2016, wundervald2022, dorie2022}, M-BART models have yet to be implemented in a multilevel multiple imputation context. Thus, my research question will be: \textit{Can multivariate imputation by chained equations through a multilevel bayesian additive regression trees model improve the bias, variance and coverage of the estimates in a multilevel context compared to current practices?} Given the success of non-parametric models in single-level multiple imputation, I anticipate that employing multilevel BART models in a multilevel missing data context will reduce bias, accurately model variance, and improve estimate coverage compared to classical multilevel imputation through \textit{2l.pmm, 2l.lmer, 2l.pan, 2l.jomo, rf} and single-level \textit{pmm} and complete case analysis in the R-package MICE \citep{buuren2011}.

This research report is organised as follows: in section 2, will contain some theoretical background and describe the methods in which I will implement the M-BART model in a multilevel imputation context and Section 3 will provide some preliminary results.

\section{Method}

\subsection{Theoretical background}
Bayesian Additive Regression Trees (BART) is a sum-of-trees model proposed by Chipman et al. \citep{chipman2010}. Regression trees are its building blocks \citep{chipman2010, hill2020, james2021}. Regression trees model non-linearities well and automatically through recursive binary partitioning of the predictor space \citep{hill2020, burgette2010}. Recursive binary partitioning doesn't assume a specific data form; it divides the predictor space to maximize variance explanation by automatically identifying best fitting splits \citep{hastie2017, james2021, salditt2023}. BART models can be described as:

\begin{subequations}
\label{eq:BART}
\begin{align}
y_i &= f(\textbf{x}_i) + \epsilon_i, \tag{1.1} \\
y_i &= g(\textbf{x}_{i}, T_{1}, M_{1})+ g(\textbf{x}_{i}, T_{2}, M_{2}) + \dots + g(\textbf{x}_{i}, T_{k}, M_{k}) + \epsilon_i, \tag{1.2}
\end{align}
\end{subequations}

where $y_i$ is the outcome variable for person \textit{i}, $f(\textbf{x}_i)$ is the sum-of-trees many regression trees, and $\epsilon_i$ is the error term; $\epsilon \sim \mathcal{N}(0,\,\sigma^{2})$. $\textbf{x}$ are the predictors included in the model, $T_{k}$ is the k\textsuperscript{th} tree and $M_{k}$ is the collection of leaf parameters within the k\textsuperscript{th} tree \citep{chipman2010, hill2020, james2021}. Next to the sum-of-trees model, BART also includes a regularization prior that constrains the size and fit of each tree so that each contributes only a small part of the overall fit to prevent overfitting \citep{chipman2010, hill2020, james2021}. The Bayesian back-fitting Markov Chain Monte Carlo (MCMC) algorithm is used to obtain estimates from BART. It updates each tree, conditional on the remaining trees, their associated parameters and $\sigma$, by fitting a new tree to the partial residuals, $r_{i}$, perturbing the tree from the previous iteration. The partial residuals, $r_{i}$, are defined as:

\begin{subequations}
\label{eq:partialresiduals}
\begin{align}
r_i &= y_i - \sum_{k' < k} \hat{f}^{b}_{k'}(x_{i}) - \sum_{k' > k} \hat{f}^{b-1}_{k'}(x_{i}), \tag{2}
\end{align}
\end{subequations}

where $\hat{f}^{b}_{k'}(x_{i})$ is the prediction of the $k'$\textsuperscript{th} tree in the $b$\textsuperscript{th} iteration for person $i$. The M-BART model including a random intercept can be identified as:

\begin{subequations}
\label{eq:M-BART}
\begin{align}
y_{ij} &= \sum_{k=1}^{m} f(\textbf{X}_{ij}; T_{k}, M_{k}) + \alpha_{j} + \epsilon_{ij}, \tag{3}
\end{align}
\end{subequations}

where, now, $y_{ij}$ is the outcome variable for person $i$ in cluster $j$ and $\alpha_{j}$ is the random intercept for cluster $j$.

\subsection{Simulation study}
I will conduct a simulation study to examine the performance of the M-BART model in a multilevel multiple imputation context through FCS. I will compare the M-BART model to the following multilevel and single level imputation methods: \textit{2l.pmm, 2l.lmer, 2l.pan, 2l.jomo, rf} and \textit{pmm} in MICE \citep{buuren2011}.
The following five factors will be varied:
\begin{enumerate}
	\item \textit{Intraclass Correlation} (ICC = 0, .05, .3 and .5)
	\item \textit{Number of clusters} (J = 30 and 50)
	\item \textit{Within-cluster sample size}  ($n_{j}$ = 5, 15, 25 and 50)
	\item \textit{The Missing At Random (MAR) and Missing Completely At Random (MCAR) data rate} (0\%, 25\% and 50\%)
	\item \textit{Within-group effect size:} (size of the regression coefficients $\beta$ = .2, .5 and .8)
\end{enumerate}
All these values are realistic in practice and/or previously proposed \citep{gulliford1999, murray2003, hox2017, grund2018, enders2018a, enders2020}. The ICC can be interpreted as the expected correlation between two randomly sampled individuals from the same group or the proportion of the total variance at level-2 \citep{gulliford2005, shieh2012, hox2011}. For each combination of varying parameters, a 1000 replicated data sets will be generated.

The simulation study will be performed in R \citep{rcoreteam2023} with the package MICE \citep{buuren2011} to perform the FCS imputations, which I will enchance by integrating a multilevel BART model. FCS multilevel imputation methods \textit{2l.pmm, 2l.lmer, 2l.pan, 2l.jomo, rf} and \textit{pmm} in MICE and complete case analysis will serve as a benchmark. The population data-generating mechanism will be based on the following multilevel model:
\begin{subequations}
\label{eq:population}
\begin{align}
y_{ij} &= \beta_{0j} + \beta_{1j}X_{1ij} + \beta_{2j}X_{2ij} + \beta_{3j}X_{3ij} + \beta_{4j}X_{4ij} + \beta_{5j}X_{5ij} + \beta_{6j}X_{6ij} + \beta_{7j}X_{7ij} + \epsilon_{ij}, \tag{4} \\
\beta_{0j} &= \gamma_{00} + \gamma_{01}Z_{1j} + \upsilon_{0j}, \tag{4.1} \\
\beta_{1j} &= \gamma_{10} + \gamma_{11}Z_{1j} + \upsilon_{1j}, \tag{4.2} \\
\beta_{2j} &= \gamma_{20} + \gamma_{21}Z_{1j} + \upsilon_{2j}, \tag{4.3} \\
\beta_{3j} &= \gamma_{30} + \gamma_{32}Z_{2j} + \upsilon_{3j}, \tag{4.4} \\
\beta_{4j} &= \gamma_{40} + \upsilon_{4j}, \tag{4.5} \\
\beta_{5j} &= \gamma_{50} + \upsilon_{5j}, \tag{4.6} \\
\beta_{6j} &= \gamma_{60} + \upsilon_{6j}, \tag{4.7} \\
\beta_{7j} &= \gamma_{70}, \tag{4.8}
\end{align}
\end{subequations}
where $y_{ij}$ is a continuous level 1 outcome variable for person $i$ in group $j$ and $Z_{1j}$ and $Z_{2j}$ are continuous level 2 variables. The random intercept $\beta_{0j}$ is determined by the grand mean $\gamma_{00}$, the group effect $\gamma_{01}Z_{1j}$ and the group-level random residuals $\upsilon_{0j}$. The regression coefficients $\beta_{1j}$, $\beta_{2j}$, and $\beta_{3j}$ for the continuous variables $X_{1ij}$, $X_{2ij}$, and $X_{3ij}$ depend on the the intercepts $\gamma_{10}$, $\gamma_{20}$, and $\gamma_{30}$, the cross-level interactions $\gamma_{11}Z_{1j}$, $\gamma_{21}Z_{1j}$, and $\gamma_{32}Z_{2j}$, and the random slopes $\upsilon_{1j}$, $\upsilon_{2j}$, and $\upsilon_{3j}$. The regression coefficients $\beta_{4j}$, $\beta_{5j}$ and $\beta_{6j}$ are determined by the intercepts $\gamma_{40}$, $\gamma_{50}$ and $\gamma_{60}$ and the random slopes $\upsilon_{4j}$, $\upsilon_{5j}$ and $\upsilon_{6j}$. The regression coefficient $\beta_{7j}$ is determined by the intercept $\gamma_{70}$. The residuals and random slopes $\upsilon_{0j}$, $\upsilon_{1j}$, $\upsilon_{2j}$, $\upsilon_{3j}$, $\upsilon_{4j}$, $\upsilon_{5j}$, $\upsilon_{6j}$ and $\epsilon_{ij}$ follow a zero-mean normal distribution. The variance of $\upsilon_{0j}$, the group-level random residuals, were scaled such that the ICC value was as specified. $\upsilon_{1j}$, $\upsilon_{2j}$, $\upsilon_{3j}$, $\upsilon_{4j}$, $\upsilon_{5j}$, and $\upsilon_{6j}$ all have a variance of 1. $X_1$, $X_2$, $X_3$, $X_4$, $X_5$, $X_6$ and $X_7$ are multivariate normally distributed: $\mathbf{X}_{ij} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, with $\boldsymbol{\mu} = (0, 0, 0, 0, 0, 0, 0)$ and $diag(\boldsymbol{\Sigma}) = (6.25, 9, 4, 11.56, 4, 2.5, 19.36)$ with no covariances. The level-2 variables $Z_1$ and $Z_2$ are also multivariate normally distributed:  $\mathbf{Z}_{j} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, with $\boldsymbol{\mu} = (0, 0)$ and $diag(\boldsymbol{\Sigma}) = (1, 2.56)$ with no covariances.

The estimates will be evaluated on their relative bias (the difference between the average estimate and the true value), modeled variance and the 95\% confidence interval coverage \citep{oberman2023}, which will be calculated as follows \citep{morris2019}:
\begin{subequations}
\label{eq:evaluations}
\begin{align}
Bias &= \frac{1}{n_{\text{sim}}} \sum_{i=1}^{n_{\text{sim}}} (\hat{\theta}_i - \theta), \tag{5} \\
% MSE &= \frac{1}{n_{\text{sim}}} \sum_{i=1}^{n_{\text{sim}}} (\hat{\theta}_i - \theta)^{2}, \tag{6} \\
Coverage &= \frac{1}{n_{\text{sim}}} \sum_{i=1}^{n_{\text{sim}}} I({\theta} \in [\hat{\theta}_{\text{min},i}, \hat{\theta}_{\text{max},i}]), \tag{6} \\
SE &= \frac{1}{n_{\text{sim}}} \sum_{i=1}^{n_{\text{sim}}} \sqrt{\frac{\sigma^2_{\hat{\theta}_i}}{N}}, \tag{7}
\end{align}
\end{subequations}
where $\hat{\theta}_i$ is the estimated parameter, $\theta$ is the true value, $n_{\text{sim}}$ is the number of simulated data sets, $\hat{\theta}_{\text{min},i}$ and $\hat{\theta}_{\text{max},i}$ are the lower and upper bounds of the 95\% confidence interval, $\sigma^2_{\hat{\theta}_i}$ is the variance of the estimated parameter and $N$ is the size of each simulated data set.

In this research report, the preliminary results will pertain to the evaluation of different BART models in terms of the bias and MSE of the estimates. The MSE will be calculated as follows \citep{morris2019}:
\begin{subequations}
\label{eq:MSE}
\begin{align}
MSE &= \frac{1}{n_{\text{sim}}} \sum_{i=1}^{n_{\text{sim}}} (\hat{\theta}_i - \theta)^{2}, \tag{8}
\end{align}
\end{subequations}
where $\hat{\theta}_i$ is the estimated parameter, $\theta$ is the true value, and $n_{\text{sim}}$ is the number of simulated data sets. In order to reduce computational time, 6 data sets will be simulated for each scenario. Furthermore, since missing data mechanisms are not relevant for this research report, they will not be incorporated in the simulated data sets. 576 data sets will be simulated in total.
4 different models will be compared: a single level BART, single level BART with group-dummys, a multilevel BART model incorporating a random intercept \citep{chen2020, wagner2020, tan2016, wundervald2022}, and a multilevel BART model combined with Stan to model the random parts of the models \citep{dorie2022}. The first three models will be perfomed in R \citep{rcoreteam2023} with the package \textit{dbarts} \citep{dorie2023} and the last will performed with the R-package \textit{stan4bart} \citep{dorie2023a}. The default arguments from the function \textit{rbart\_vi} will be used for all models, as well as the default priors.

\section{Results}

\newpage
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% \usepackage{lscape}
\begin{landscape}
\begin{table}[]
\caption{Bias of the estimates for every data set and ICC value}
\label{tab:bias}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lll|rrrr|rrrr|rrrr|rrrr}
\hline
\multicolumn{3}{l|}{\textbf{Dataset}} & \multicolumn{4}{c|}{\textbf{ICC = 0}}                                                                                                                 & \multicolumn{4}{c|}{\textbf{ICC = .05}}                                                                                                               & \multicolumn{4}{c|}{\textbf{ICC = .3}}                                                                                                                & \multicolumn{4}{c}{\textbf{ICC = .5}}                                                                                                                \\ \hline
Number of groups  & Groupsize  & Beta & \multicolumn{1}{l}{\textbf{bart}} & \multicolumn{1}{l}{\textbf{gbart}} & \multicolumn{1}{l}{\textbf{rbart}} & \multicolumn{1}{l|}{\textbf{stan4bart}} & \multicolumn{1}{l}{\textbf{bart}} & \multicolumn{1}{l}{\textbf{gbart}} & \multicolumn{1}{l}{\textbf{rbart}} & \multicolumn{1}{l|}{\textbf{stan4bart}} & \multicolumn{1}{l}{\textbf{bart}} & \multicolumn{1}{l}{\textbf{gbart}} & \multicolumn{1}{l}{\textbf{rbart}} & \multicolumn{1}{l|}{\textbf{stan4bart}} & \multicolumn{1}{l}{\textbf{bart}} & \multicolumn{1}{l}{\textbf{gbart}} & \multicolumn{1}{l}{\textbf{rbart}} & \multicolumn{1}{l}{\textbf{stan4bart}} \\ \hline
30                & 15         & 0.2  & -0.00022                          & -0.00095                           & -0.00374                           & -0.00041                                & -0.00553                          & -0.00165                           & 0.00616                            & 0.00078                                 & -0.00323                          & -0.00365                           & 0.00290                            & 0.00164                                 & 0.00243                           & 0.00184                            & 0.00253                            & 0.00163                                \\
30                & 15         & 0.5  & -0.00111                          & -0.00270                           & 0.00091                            & 0.00196                                 & -0.00834                          & 0.00298                            & 0.00093                            & -0.00245                                & 0.00128                           & -0.00162                           & 0.00338                            & -0.00279                                & 0.00452                           & 0.00730                            & 0.00734                            & 0.00094                                \\
30                & 15         & 0.8  & -0.00068                          & -0.00289                           & -0.00203                           & -0.00021                                & 0.00268                           & 0.00617                            & -0.00133                           & -0.00166                                & -0.00731                          & 0.00272                            & -0.00304                           & -0.00105                                & -0.01136                          & -0.00108                           & -0.00100                           & -0.00134                               \\
30                & 35         & 0.2  & 0.00123                           & 0.00147                            & 0.00044                            & 0.00007                                 & 0.00030                           & -0.00222                           & 0.00072                            & -0.00092                                & -0.00316                          & -0.00220                           & -0.00240                           & 0.00001                                 & 0.00115                           & 0.00286                            & 0.00313                            & 0.00033                                \\
30                & 35         & 0.5  & 0.00333                           & 0.00271                            & -0.00050                           & 0.00031                                 & 0.00135                           & 0.00062                            & 0.00025                            & -0.00027                                & 0.00205                           & 0.00061                            & -0.00051                           & 0.00151                                 & -0.00250                          & 0.00129                            & -0.00302                           & -0.00106                               \\
30                & 35         & 0.8  & -0.00120                          & -0.00075                           & 0.00081                            & 0.00025                                 & -0.00154                          & -0.00051                           & -0.00221                           & -0.00098                                & 0.00500                           & -0.00113                           & -0.00036                           & 0.00059                                 & 0.00025                           & -0.00233                           & 0.00526                            & -0.00035                               \\
30                & 50         & 0.2  & 0.00038                           & 0.00215                            & -0.00139                           & 0.00014                                 & -0.00071                          & 0.00222                            & -0.00196                           & 0.00105                                 & -0.00060                          & -0.00027                           & -0.00314                           & -0.00075                                & -0.00168                          & -0.00091                           & -0.00130                           & -0.00105                               \\
30                & 50         & 0.5  & 0.00084                           & 0.00123                            & 0.00104                            & 0.00000                                 & 0.00032                           & -0.00019                           & 0.00093                            & 0.00080                                 & -0.00048                          & 0.00328                            & 0.00118                            & 0.00138                                 & 0.00288                           & 0.00116                            & -0.00101                           & -0.00047                               \\
30                & 50         & 0.8  & -0.00077                          & 0.00069                            & -0.00004                           & 0.00050                                 & -0.00250                          & -0.00263                           & 0.00120                            & -0.00140                                & 0.00186                           & 0.00219                            & 0.00104                            & -0.00049                                & 0.00367                           & 0.00249                            & 0.00413                            & -0.00043                               \\
30                & 5          & 0.2  & -0.00084                          & -0.00293                           & 0.00122                            & -0.00103                                & 0.01028                           & -0.00232                           & 0.00343                            & -0.00260                                & -0.02377                          & -0.02612                           & -0.00376                           & -0.01058                                & 0.00902                           & 0.00195                            & 0.00815                            & 0.00276                                \\
30                & 5          & 0.5  & -0.00029                          & 0.00189                            & -0.00160                           & -0.00330                                & -0.00607                          & -0.01355                           & -0.00227                           & -0.00851                                & -0.00806                          & -0.00296                           & -0.01297                           & -0.00138                                & 0.00405                           & 0.01207                            & 0.00680                            & 0.00411                                \\
30                & 5          & 0.8  & -0.00406                          & -0.00375                           & -0.00665                           & -0.00460                                & -0.00391                          & 0.00158                            & -0.00973                           & 0.00778                                 & -0.01015                          & -0.00591                           & 0.00118                            & -0.00150                                & -0.01604                          & 0.01273                            & 0.01258                            & -0.00038                               \\
50                & 15         & 0.2  & -0.00161                          & 0.00054                            & -0.00152                           & 0.00012                                 & -0.00311                          & 0.00596                            & -0.00162                           & -0.00059                                & 0.00080                           & -0.00288                           & 0.00299                            & -0.00049                                & 0.00228                           & -0.00159                           & 0.00184                            & 0.00029                                \\
50                & 15         & 0.5  & -0.00202                          & 0.00119                            & 0.00026                            & 0.00025                                 & 0.00088                           & -0.00140                           & 0.00226                            & 0.00001                                 & -0.00026                          & -0.00740                           & -0.00164                           & -0.00053                                & 0.00259                           & 0.00301                            & -0.00175                           & -0.00152                               \\
50                & 15         & 0.8  & 0.00195                           & -0.00013                           & -0.00033                           & -0.00125                                & -0.00175                          & 0.00122                            & 0.00282                            & 0.00150                                 & 0.00246                           & -0.00029                           & -0.00415                           & -0.00105                                & 0.00224                           & 0.00372                            & -0.00552                           & -0.00140                               \\
50                & 35         & 0.2  & -0.00170                          & 0.00023                            & 0.00057                            & 0.00038                                 & -0.00066                          & -0.00104                           & -0.00041                           & 0.00018                                 & -0.00094                          & -0.00260                           & 0.00340                            & -0.00030                                & 0.00263                           & 0.00112                            & -0.00155                           & 0.00007                                \\
50                & 35         & 0.5  & -0.00014                          & 0.00068                            & -0.00028                           & -0.00047                                & -0.00085                          & 0.00136                            & -0.00095                           & -0.00084                                & -0.00175                          & 0.00209                            & -0.00097                           & 0.00134                                 & 0.00120                           & -0.00265                           & -0.00030                           & 0.00068                                \\
50                & 35         & 0.8  & -0.00019                          & 0.00174                            & 0.00257                            & -0.00068                                & -0.00357                          & 0.00356                            & 0.00219                            & -0.00005                                & 0.00322                           & -0.00305                           & -0.00283                           & -0.00001                                & 0.00105                           & -0.00044                           & -0.00144                           & 0.00112                                \\
50                & 50         & 0.2  & 0.00058                           & 0.00051                            & -0.00032                           & 0.00055                                 & 0.00069                           & 0.00199                            & -0.00008                           & 0.00039                                 & 0.00172                           & -0.00092                           & 0.00003                            & 0.00011                                 & 0.00080                           & 0.00050                            & -0.00020                           & -0.00054                               \\
50                & 50         & 0.5  & 0.00059                           & -0.00046                           & -0.00138                           & 0.00001                                 & 0.00115                           & 0.00305                            & 0.00041                            & 0.00007                                 & 0.00029                           & -0.00020                           & -0.00066                           & -0.00037                                & 0.00388                           & 0.00009                            & 0.00382                            & 0.00009                                \\
50                & 50         & 0.8  & -0.00045                          & 0.00189                            & 0.00005                            & 0.00024                                 & 0.00080                           & -0.00216                           & 0.00415                            & -0.00144                                & -0.00236                          & 0.00188                            & -0.00130                           & 0.00017                                 & 0.00009                           & -0.00027                           & 0.00132                            & 0.00011                                \\
50                & 5          & 0.2  & 0.00159                           & 0.00166                            & 0.00549                            & 0.00475                                 & -0.00347                          & 0.00158                            & -0.00341                           & -0.00146                                & 0.00378                           & 0.00990                            & 0.00002                            & 0.00329                                 & -0.00214                          & 0.00794                            & 0.00126                            & -0.00384                               \\
50                & 5          & 0.5  & -0.00043                          & 0.00356                            & -0.00151                           & 0.00012                                 & -0.00124                          & -0.00274                           & 0.00491                            & -0.00306                                & 0.00170                           & 0.00957                            & -0.00043                           & 0.00186                                 & 0.00539                           & -0.00318                           & -0.00009                           & -0.00370                               \\
50                & 5          & 0.8  & 0.00258                           & -0.00034                           & -0.00233                           & -0.00332                                & 0.00708                           & -0.00626                           & 0.00145                            & 0.00129                                 & -0.00645                          & -0.00138                           & 0.00793                            & -0.00054                                & 0.00120                           & 0.00354                            & -0.00348                           & 0.00081
\end{tabular}%
}
\end{table}
\end{landscape}

\newpage
\bibliography{thesis}
\bibliographystyle{apalike}

\end{document}

