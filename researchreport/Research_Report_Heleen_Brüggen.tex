\documentclass[10pt, a4paper, titlepage]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx, latexsym}
\usepackage{titling}
\setlength{\droptitle}{-25em}
\renewcommand{\maketitlehooka}{\Large}
\usepackage{setspace}
\usepackage{amssymb, amsmath, amsthm}
\usepackage[export]{adjustbox}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{epstopdf}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\usepackage{titling}
\usepackage[sort&compress,square,comma,authoryear]{natbib}
\hypersetup{
    pdftitle={Research Report Heleen Br端ggen},
    pdfauthor={Heleen Br端ggen},
    pdfsubject={Research Report Heleen Br端ggen},
    pdfkeywords={},
    bookmarksnumbered=true,
    bookmarksopen=true,
    bookmarksopenlevel=1,
    colorlinks=false,
    pdfstartview=Fit,
    pdfpagemode=UseNone
}

\singlespacing

\begin{document}
\begin{titlingpage}
\begin{center}
\Huge\textbf{Master Research Report:  \\ Multilevel Multivariate Imputation by Chained Equations through Bayesian Additive Regression Trees} \\
\Large\textit{Methodology and Statistics for the Behavioural, Biomedical and Social Sciences}

\vspace{.5cm}

\normalsize\textit{Heleen Br端ggen}

\vspace{15cm}

\begin{minipage}{0.5\textwidth}
\begin{flushleft}

\textbf{Word count:} \\
\textbf{Candidate Journal:} \\
\textbf{FETC Case Number:} \\
\textbf{Supervisors:} \\
MSc. T. Volker \\
Dr. G. Vink \\
 MSc. H. Oberman
\end{flushleft}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\begin{flushright}

1384 \\
Computational Statistics \& Data Analysis \\
23-1778 \\
------------------------\\
Utrecht University \\
Utrecht University \\
Utrecht University
\end{flushright}
\end{minipage}

\end{center}
\end{titlingpage}

\newpage

\section{Introduction}

Incomplete data is a common challenge in many fields of research. A frequent approach for dealing with incomplete data is listwise deletion, also known as complete-case analysis, which is to remove all incomplete cases from the data. However, this could possibly lead to biased results if the data is not Missing Completely At Random (MCAR), meaning the cause of the missing data is unrelated to the data \citep{buurenFlexibleImputationMissing2018, kang2013, enders2017, austin2021, rubin1976}. Furthermore, other approaches to dealing with incomplete data include: pairwise deletion, mean imputation and regression imputation, which also yield biased results \citep{buurenFlexibleImputationMissing2018}. Pairwise deletion, also known as available-case analysis, is to remove all incomplete cases from the analysis when considering a specific pair of variables. Pairwise deletion leads to unbiased results when correlation between variables are low and the data is MCAR. Mean imputation is to replace missing values with the mean of the observed values. Mean imputation will bias almost all estimates except the mean when the data is not MCAR. Regression imputation is to replace missing values with the predicted values from a regression model and is unbiased when the data is Missing At Random (MAR), meaning that the missing data is related to the observed data \citep{rubin1976}, and the factor influencing the missingness is present in the data \citep{buurenFlexibleImputationMissing2018}. So, in order to use these ad hoc strategies to deal with incomplete data correctly, the missing data mechanism should be carefully determined. However, determining the missing data mechanism is often difficult and MCAR is in practise an unrealistic assumption \citep{buurenFlexibleImputationMissing2018}. We can imagine that when a system becomes more complex, thus increases the amount of parameters, testing these assumptions also increases in complexity.

One of these complex systems are multilevel data structures. Multilevel data is hierarchically structured, where, for example, students are nested within schools, or patients are nested within hospitals \citep{hox2017}. Thus, in these types of data sets there are level-1 and level-2 variables. Level-1 variables relate to the individual within a class and level-2 variables relate to the class as a whole. Multilevel data sets can contain both level-1 and level-2 variables, random intercepts, random slopes, and cross-level interactions \citep{hox2017}. There are three ad-hoc strategies for dealing with multilevel missing data \citep{buurenFlexibleImputationMissing2018}. The first is listwise deletion, which

ignoring the multilevel structure and fixed effect imputation: adding group dummy variables representing the group effects \citep{ludtke2017, enders2016}. However, these strategies produce biased estimates of variance components and multilevel regression coefficients \citep{ludtke2017}.

In a missing data context, imputation models should at least be as general as the analysis model and preferably all-encompassing \citep{grund2018}.

Multiple imputation (MI) is considered a valid method for dealing with incomplete data \citep{mistlerComparisonJointModel2017, buurenFlexibleImputationMissing2018, enders2017, burgette2010, austin2021, audigier2018, vanbuuren2007, grund2021, hughes2014}. MI imputes each missing value more than once, thereby considering necessary variation associated with the missingness problem. The multiply imputed data sets are analyzed, and the corresponding inferences are pooled according to Rubin's rules \citep{buurenFlexibleImputationMissing2018, austin2021, rubin1987}.
Generally, multiple imputation operates under two frameworks: joint modeling (JM) and fully conditional specification (FCS) \citep{mistlerComparisonJointModel2017, buurenFlexibleImputationMissing2018, enders2018, enders2018a, hughes2014}. JM employs a multivariate data distribution and regresses incomplete variables on complete variables to impute missing values. FCS, or chained equations, iteratively imputes one variable with missing values at a time through conditional univariate distributions regressing an incomplete variable complete and previously imputed variables \citep{mistlerComparisonJointModel2017, buurenFlexibleImputationMissing2018, enders2018, enders2018a, hughes2014}.

JM and FCS are extended to a multilevel or hierarchical context.%Multilevel data is hierarchically structured, where, for example, students are nested within schools, or patients are nested within hospitals \citep{hox2017}.
JM is extend by defining a multivariate linear mixed model. FCS is extended by defining a series of univariate linear mixed models \citep{mistlerComparisonJointModel2017}.

Two ad-hoc strategies for dealing with multilevel missing data are: ignoring the multilevel structure and fixed effect imputation: adding group dummy variables representing the group effects \citep{ludtke2017, enders2016}. However, these strategies produce biased estimates of variance components and multilevel regression coefficients \citep{ludtke2017}.

Currently, the implementation of JM and FCS in a multilevel context are appropriate in a two-level random intercept context with normally distributed data. However, they differ beyond that: JM is more capable of handling within- and between- cluster relationships, random intercepts and incomplete categorical variables, while FCS is better suited for random slopes and restricted to normally distributed variables \citep{enders2016}. Also, they differ in their handling of missing level-2 data. Overall, FCS is believed to be more flexible than JM \citep{audigier2018} and, thus, may be better suited for multilevel data.

%Currently, the specifications of the imputation models in a multilevel context are quite complex \citep{buurenFlexibleImputationMissing2018}: they should at least be as general as the analysis model \citep{grund2018} and preferably all-encompassing.
However, the complexity of the multilevel analysis model is built step-wise with non-linearities \citep{hox2017} and a very complex model might not converge \citep{buurenFlexibleImputationMissing2018}. Within the package MICE \citep{buuren2011} the user has to specify conditional models for all variables with missing values, which can become quite complex in a multilevel setting \citep{buurenFlexibleImputationMissing2018, burgette2010}. MICE implements the following methods in the FCS framework: \textit{21.bin, 21.lmer, 21.pan, 21.continuous, 21.jomo, 21.glm.norm, 21.norm, 21.2stage.norm, 21.pmm, and 21.2stage.pmm}.

Bayesian Additive Regression Trees (BART) is a sum-of-trees model proposed by Chipman et al. \citep{chipman2010}. Regression trees are its building blocks \citep{chipman2010, hill2020, james2021}. Regression trees model non-linearities well and automatically through recursive binary partitioning of the predictor space \citep{hill2020, burgette2010}. Recursive binary partitioning doesn't assume a specific data form; it divides the predictor space to maximize variance explanation by automatically identifying best fitting splits \citep{hastie2017, james2021, salditt2023}. BART models can be described as:

\begin{subequations}
\label{eq:BART}
\begin{align}
y_i &= f(\textbf{x}_i) + \epsilon_i, \tag{1.1} \\
y_i &= g(\textbf{x}_{i}, T_{1}, M_{1})+ g(\textbf{x}_{i}, T_{2}, M_{2}) + \dots + g(\textbf{x}_{i}, T_{k}, M_{k}) + \epsilon_i, \tag{1.2}
\end{align}
\end{subequations}

where $y_i$ is the outcome variable for person \textit{i}, $f(\textbf{x}_i)$ is the sum-of-trees many regression trees, and $\epsilon_i$ is the error term; $\epsilon \sim \mathcal{N}(0,\,\sigma^{2})$. $\textbf{x}$ are the predictors included in the model, $T_{k}$ is the k\textsuperscript{th} tree and $M_{k}$ is the collection of leaf parameters within the k\textsuperscript{th} tree \citep{chipman2010, hill2020, james2021}. Next to the sum-of-trees model, BART also includes a regularization prior that constrains the size and fit of each tree so that each contributes only a small part of the overall fit to prevent overfitting \citep{chipman2010, hill2020, james2021}. The Bayesian back-fitting Markov Chain Monte Carlo (MCMC) algorithm is used to obtain estimates from BART. It updates each tree, conditional on the remaining trees, their associated parameters and $\sigma$, by fitting a new tree to the partial residuals, $r_{i}$, perturbing the tree from the previous iteration. The partial residuals, $r_{i}$, are defined as:

\begin{subequations}
\label{eq:partialresiduals}
\begin{align}
r_i &= y_i - \sum_{k' < k} \hat{f}^{b}_{k'}(x_{i}) - \sum_{k' > k} \hat{f}^{b-1}_{k'}(x_{i}), \tag{2}
\end{align}
\end{subequations}

where $\hat{f}^{b}_{k'}(x_{i})$ is the prediction of the $k'$\textsuperscript{th} tree in the $b$\textsuperscript{th} iteration for person $i$.

In a single-level imputation context, the use of tree-based models like regression trees, random forests or BARTs simplified imputation models and performed better than parametric methods: the estimates showed better confidence interval coverage of the population parameters, lower variance and lower bias, especially in non-linear and interactive contexts \citep{burgette2010, xu2016, silva2022}. Others have also found lower normalized root mean squared error (NRMSE), which in essence encapsulates the bias of the imputations, when imputing with an random forest algorithm compared to MICE and KNN imputation \citep{stekhoven2012, waljee2013}. Furthermore, they also found that the algorithm reduced computational time and could handle multivariate data consisting of both continuous and categorical data simultaneously.

BART models have also been implemented in a multilevel prediction context. However, multilevel-BART models (M-BART) have predominantly only been implemented with random intercepts and no random slopes and cross-level interactions \citep{chen2020, wagner2020, tan2016, wundervald2022}. The M-BART model including a random intercept can be identified as:

\begin{subequations}
\label{eq:M-BART}
\begin{align}
y_{ij} &= \sum_{k=1}^{m} f(\textbf{X}_{ij}; T_{k}, M_{k}) + \alpha_{j} + \epsilon_{ij}, \tag{3}
\end{align}
\end{subequations}

where, now, $y_{ij}$ is the outcome variable for person $i$ in cluster $j$ and $\alpha_{j}$ is the random intercept for cluster $j$. \citet{wagner2020} and have found that this random intercept M-BART model provided better estimates with a lower Mean Squared Error (MSE) compared to a parametric multilevel model, \citet{tan2016} found higher Area Under the Curve values, and \citet{chen2020} found better estimates and better coverage compared to parametric models and a single-level BART model. Other researchers modeled the random intercept as an extra split on each terminal node within the BART algorithm and found a lower MSE compared to a standard BART model and parametric multilevel models \citep{wundervald2022}. Dorie et al. developed a multilevel BART model that included random intercepts and random slopes by combining BART with the Stan algorithm \citep{dorie2022}. However, the random intercept and slope are modeled by Stan, which is a parametric method. Their results showed that their algorithm `stan4bart` showed better coverage of the population value and lower Root Mean Squared Error (RMSE) compared to BART models with varying intercept, BART models ignoring the multilevel structure, Bayesian Causal Forests (BCF), and parametric multilevel models.

In spite of these promising findings: tree-based model performing well in single-level imputation context \citep{burgette2010, xu2016, silva2022, stekhoven2012, waljee2013} and M-BART models performing well in a multilevel prediction context \citep{chen2020, wagner2020, tan2016, wundervald2022, dorie2022}, M-BART models have yet to be implemented in a multilevel multiple imputation context. Thus, my research question will be: \textit{Can multivariate imputation by chained equations through a multilevel bayesian additive regression trees model improve the bias, variance and coverage of the estimates in a multilevel context compared to current practices?} Given the success of non-parametric models in single-level multiple imputation, I anticipate that employing multilevel BART models in a multilevel missing data context will reduce bias, accurately model variance, and improve estimate coverage compared to classical multilevel imputation through \textit{21.pmm, 21.lmer, 21.pan, 21.jomo, rf} and \textit{pmm} in MICE.

This research report is organised as follows: in section 2, I will describe the methods in which I will implement the M-BART model in a multilevel imputation context and Section 3 will provide some preliminary results.

\section{Method}

\subsection{Theoretical background}

\subsection{Simulation study}

\section{Results}

\newpage
\bibliography{thesis}
\bibliographystyle{apalike}

\end{document}

