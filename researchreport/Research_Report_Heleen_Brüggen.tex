\documentclass[10pt, a4paper, titlepage]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx, latexsym}
\usepackage{titling}
\setlength{\droptitle}{-25em}
\renewcommand{\maketitlehooka}{\Large}
\usepackage{setspace}
\usepackage{amssymb, amsmath, amsthm}
\usepackage[export]{adjustbox}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{epstopdf}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\usepackage{titling}
\usepackage{multirow}
\usepackage[labelfont=bf]{caption}
\usepackage[table,xcdraw]{xcolor}
\usepackage{colortbl}
\usepackage{lscape}
\usepackage{float}
\usepackage[sort&compress,square,comma,authoryear]{natbib}
\hypersetup{
    pdftitle={Research Report Heleen Br端ggen},
    pdfauthor={Heleen Br端ggen},
    pdfsubject={Research Report Heleen Br端ggen},
    pdfkeywords={},
    bookmarksnumbered=true,
    bookmarksopen=true,
    bookmarksopenlevel=1,
    colorlinks=false,
    pdfstartview=Fit,
    pdfpagemode=UseNone
}

\singlespacing

\begin{document}
\begin{titlingpage}
\begin{center}
\Huge\textbf{Master Research Report:  \\ Multilevel Multivariate Imputation by Chained Equations through Bayesian Additive Regression Trees} \\
\Large\textit{Methodology and Statistics for the Behavioural, Biomedical and Social Sciences}

\vspace{.5cm}

\normalsize\textit{Heleen Br端ggen}

\vspace{15cm}

\begin{minipage}{0.5\textwidth}
\begin{flushleft}

\textbf{Word count:} \\
\textbf{Candidate Journal:} \\
\textbf{FETC Case Number:} \\
\textbf{Supervisors:} \\
MSc. T. Volker \\
Dr. G. Vink \\
 MSc. H. Oberman
\end{flushleft}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\begin{flushright}

3461 \\
Computational Statistics \& Data Analysis \\
23-1778 \\
------------------------\\
Utrecht University \\
Utrecht University \\
Utrecht University
\end{flushright}
\end{minipage}

\end{center}
\end{titlingpage}

\newpage

\section{Introduction}

Incomplete data is a common challenge in many fields of research. Frequently, ad hoc strategies are used to deal with missing data, such as listwise deletion, pairwise deletion, mean imputation and regression imputation. However, these strategies typically lead to erroneous inferences in a realistic situation. The estimated may be biased and variances in the data are no longer correct \citep{buurenFlexibleImputationMissing2018, kang2013, enders2017, austin2021}. Rubin defined three types of missing data mechanisms: Missing Completely At Random (MCAR) where the cause of the missing data is unrelated to the missing data, Missing At Random (MAR) where the missing data is related to the observed data, and Missing Not At Random (MNAR) where the missing data is related to unobserved data \citep{rubin1976}.

Multiple imputation (MI) is considered a valid method for dealing with incomplete data and allows us the separate the missing data problem from the analysis problem \citep{mistlerComparisonJointModel2017, buurenFlexibleImputationMissing2018, enders2017, burgette2010, austin2021, audigier2018, vanbuuren2007, grund2021, hughes2014}. MI is used to impute each missing value in the data set more than once given the observed data, thereby considering necessary variation associated with the missingness problem. The multiply imputed data sets are analyzed, and the corresponding inferences are pooled according to Rubin's rules \citep{buurenFlexibleImputationMissing2018, austin2021, rubin1987, carpenter2013}. The models used to impute the missing data, the imputation models, should be at least as general as the analysis model and preferably all-encompassing \citep{grund2018, enders2018, meng1994multiple, bartlett2015, grund2016}. Otherwise, the imputation model will not capture every aspect of the data and the completed data sets cannot be properly analysed. So, when the complexity of data increases, specifying the imputation models becomes more difficult \citep{grund2018, buurenFlexibleImputationMissing2018}.

MI has been implemented in a multilevel data context \citep{mistlerComparisonJointModel2017, enders2018, enders2018a, enders2020, buurenFlexibleImputationMissing2018, taljaard2008, enders2016, resche-rigon2018, audigier2018, dong2023, grund2016, grund2018a, grund2018, ludtke2017, grund2021, quartagno2022}, where this problem of specifying imputation models as general as the analysis model becomes magnified. Multilevel data is hierarchically structured, where, for example, students are nested within schools, or patients are nested within hospitals \citep{hox2017, hox2011}. Thus, in these types of data sets there are level-1 and level-2 variables. Level-1 variables relate to the individual within a class and level-2 variables relate to the class as a whole. The recommended statistical technique to analyzing these models are multilevel models as it accounts for the specific dependencies in the multilevel data sets \citep{hox2017, hox2011, ludtke2017}. It can contain both level-1 and level-2 variables, random intercepts, random slopes, and cross-level interactions \citep{hox2017, hox2011}. Furthermore, the complexity of the multilevel analysis model is built step-wise with non-linearities \citep{hox2017, hox2011}. Thus, when defining imputation models for a multilevel data set one needs to account for the hierarchical structure, as well as the complicated non-linearities from cross-level interactions. Since ignoring the multilevel structure and imputing the data as if it were a single-level data set will underestimate the Intra-Class Correlation (ICC) \citep{buurenFlexibleImputationMissing2018, ludtke2017, taljaard2008, hox2011}. The ICC can be interpreted as the expected correlation between two randomly sampled individuals from the same group or the proportion of the total variance at level-2 \citep{gulliford2005, shieh2012, hox2011}. So, defining these models is quite challenging \citep{buurenFlexibleImputationMissing2018, burgette2010, hox2011}.

Generally, multiple imputation operates under two frameworks: joint modeling (JM) and fully conditional specification (FCS) \citep{mistlerComparisonJointModel2017, buurenFlexibleImputationMissing2018, enders2018, enders2018a, hughes2014}. JM employs a multivariate data distribution and regresses incomplete variables on all complete variables to impute missing values. FCS, or chained equations, iteratively imputes one variable with missing values at a time through conditional univariate distributions regressing an incomplete variable on complete variables and previously imputed variables \citep{mistlerComparisonJointModel2017, buurenFlexibleImputationMissing2018, enders2016, enders2018, enders2018a, hughes2014, grund2018a}. JM and FCS use multivariate and univariate linear mixed models respectively in a multilevel context \citep{mistlerComparisonJointModel2017, enders2018, resche-rigon2018}, accounting for the hierarchical structure. Thus far, much focus has been on mulitlevel JM and FCS including random intercepts and slopes, but no cross-level interactions \cite{grund2018a, grund2016, enders2018, enders2018a, enders2020, enders2016}. Overall, FCS is believed to be more flexible than JM \citep{audigier2018, burgette2010, vanbuuren2007, grund2018a} and, thus, may be better suited for multilevel data. \citet{grund2018} recommend using FCS to impute a multilevel data set with random intercepts, slopes and cross-level interactions or non-linear terms. FCS uses 'passive imputation' or treats interactions as 'just another variable' by defining an imputation model for interactions. On the other hand, JM does not account for cross-level interactions at all \citep{buurenFlexibleImputationMissing2018, grund2018}. Still, the imputation model of multilevel models including cross-level interaction or non-linear terms in FCS is still a very complicated process \citep{grund2021, grund2018}.

Using non-parametric tree-based models as imputation models might solve this problem. In a single-level imputation context, the use of tree-based, non-parametric models like regression trees, random forests or Bayesian Additive Regression Trees (BART) simplified imputation models and performed better than parametric methods: the imputations showed better confidence interval coverage of the population parameters, lower variance and lower bias, especially in non-linear and interactive contexts \citep{burgette2010, xu2016, silva2022}. \citet{waljee2013} also found lower imputation error when imputing with a random forest algorithm compared to MICE, KNN and mean imputation and \citet{stekhoven2012} found that the algorithm reduced computational time and could handle multivariate data consisting of both continuous and categorical data simultaneously.

Tree-based models use recursive partitioning to split the data into smaller subgroups based on the predictor variables maximizing the homogeneity of the subgroups. Tree-based models are non-parametric, which means that they do not assume a specific distribution of the data. Thus, they can handle non-linear relationships and interactions between the predictor variables well. Furthermore they handle continuous and categorical variables simultaneously \citep{hill2020, burgette2010, lin2019, chipman2010, james2021, salditt2023, breiman1984}.

BART models have also been implemented in a multilevel prediction context. However, multilevel-BART models (M-BART) have predominantly been implemented with random intercepts and no random slopes and cross-level interactions \citep{chen2020, wagner2020, tan2016, wundervald2022}. In prediction context, \citet{wagner2020} have found that this random intercept M-BART model provided better predictions with a lower Mean Squared Error (MSE) compared to a parametric multilevel model, \citet{tan2016} found higher Area Under the Curve values compared to a single level BART model and linear random intercept model, and \citet{chen2020} found better predictions and better coverage compared to parametric models and a single-level BART model. Other researchers modeled the random intercept as an extra split on each terminal node within the BART algorithm and found a lower MSE compared to a standard BART model and parametric multilevel models \citep{wundervald2022}. \citet{dorie2022} developed a multilevel BART model that included random intercepts and random slopes by combining BART with the Stan algorithm. However, the random intercept and slope are modeled by Stan, which is a parametric method. Their results showed that their algorithm `stan4bart` showed better coverage of the population value and lower Root Mean Squared Error (RMSE) compared to BART models with varying intercept, BART models ignoring the multilevel structure, Bayesian Causal Forests (BCF), and parametric multilevel models.

In spite of these promising findings: tree-based model performing well in single-level imputation context and M-BART models performing well in a multilevel prediction context, M-BART models have yet to be implemented in a multilevel multiple imputation context. Thus, my research question will be: \textit{Can multivariate imputation by chained equations through a multilevel bayesian additive regression trees model improve the bias, variance and coverage of the estimates in a multilevel context compared to current practices?} Given the success of non-parametric models in single-level multiple imputation, I anticipate that employing multilevel BART models in a multilevel missing data context will reduce bias, accurately model variance, and improve estimate coverage compared to classical multilevel imputation through \textit{2l.pmm, 2l.lmer, 2l.pan, 2l.jomo, rf} and single-level \textit{pmm} and complete case analysis in the R-package MICE \citep{buuren2011}.

This research report is organised as follows: in section 2, will contain some theoretical background and describe the methods in which I will evaluate different M-BART models for imputation and Section 3 will provide some preliminary results.

\section{Method}

\subsection{Theoretical background}
\subsubsection{Bayesian Additive Regression Trees (BART)}
Bayesian Additive Regression Trees (BART) is a sum-of-trees model proposed by \citet{chipman2010}. Regression trees are its building blocks \citep{chipman2010, hill2020, james2021}. Regression trees divides the data into subgroups by recursively splitting the data into binary subgroups based on the predictors. The splits are chosen to maximize the homogeneity of the subgroups \citep{hastie2017, james2021, salditt2023}. Recursive binary partitioning of the predictor space doesn't assume a specific data form, making this a non-parametric model \citep{hastie2017, james2021, salditt2023} and allows regression trees to model non-linearities well and automatically \citep{hill2020, burgette2010}.

\citet{chipman2010} define the BART model as:

\begin{subequations}
\label{eq:BART}
\begin{align}
f(\textbf{x}) &= \sum^{m}_{k=1}g(\textbf{x}; T_{k}, M_{k}), \tag{1.1} \\
f(\textbf{x}) &= g(\textbf{x}, T_{1}, M_{1}) + g(\textbf{x}_{i}, T_{2}, M_{2}) + \dots + g(\textbf{x}_{i}, T_{m}, M_{m}), \tag{1.2}
\end{align}
\end{subequations}

where $f(\mathbf{x})$ is the overall fit of the model: the sum of many regression trees; $\textbf{x}$ are the predictors included in the model, $T_{k}$ is the k\textsuperscript{th} tree and $M_{k}$ is the collection of leaf parameters within the k\textsuperscript{th} tree \citep{chipman2010, hill2020, james2021}. The data are assumed to arise from a model with additive normally distributed errors: $Y = \sum^{m}_{k=1}g(\textbf{x}; T_{k}, M_{k}) + \epsilon, \epsilon \sim \mathcal{N}(0,\,\sigma^{2})$.
Next to the sum-of-trees model, BART also includes a regularization prior that constrains the size and fit of each tree so that each contributes only a small part of the overall fit to prevent overfitting \citep{chipman2010, hill2020, james2021}. BARTs can be estimated by using the Bayesian back-fitting Markov Chain Monte Carlo (MCMC) algorithm, which updates each tree, conditional on the remaining trees, their associated parameters and the residual standard deviation ($\sigma$), by fitting a new tree to the partial residuals, $r_{i}$, treating it as the data and perturbing the tree from the previous iteration. It can either grow, prune, and change a tree. Growing means adding additional splits, pruning removes splits, and changing chaning decision rules. The algorithm stops after the specified amount of iterations. The partial residuals, $r_{i}$, are defined as:

\begin{subequations}
\label{eq:partialresiduals}
\begin{align}
r_i &= y_i - \sum_{k' < k} \hat{f}^{b}_{k'}(x_{i}) - \sum_{k' > k} \hat{f}^{b-1}_{k'}(x_{i}), \tag{2}
\end{align}
\end{subequations}

where $\hat{f}^{b}_{k}(x_{i})$ is the prediction of the $k$\textsuperscript{th} tree in the $b$\textsuperscript{th} iteration for person $i$.

\subsubsection{Multilevel-BART (M-BART)}
\citet{chen2020, wagner2020} and \citet{tan2016} define a multilevel-BART (M-BART) model including a random intercept building on the work of \citet{lin2019}. The M-BART algorithm breaks down the observed variable into fixed and random components. The fixed components are modeled by a BART model and the random components are modeled by linear mixed effects model \citep{chen2020, wagner2020, tan2016}.
The M-BART model including a random intercept can be identified as:

\begin{subequations}
\label{eq:M-BART}
\begin{align}
f(\textbf{x}) &= \sum^{m}_{k=1}g(\textbf{x}; T_{k}, M_{k}) + \alpha_{j}, \tag{3}
\end{align}
\end{subequations}

where, now, $f(\textbf{x})$ is the overal fit of the model incorporating random intercept $\alpha_{j}$ for cluster $j$.

\subsection{Simulation study}
For this research report, I will conduct a simulation study to examine the performance of three M-BART models in a multilevel prediction context compared to a sinlge-level BART model.

\subsubsection{Data generating mechanism}
The population data-generating mechanism will be based on the following multilevel model:
\begin{subequations}
\label{eq:population}
\begin{align}
y_{ij} &= \beta_{0j} + \beta_{1j}X_{1ij} + \beta_{2j}X_{2ij} + \beta_{3j}X_{3ij} + \beta_{4j}X_{4ij} + \beta_{5j}X_{5ij} + \beta_{6j}X_{6ij} + \beta_{7j}X_{7ij} + \epsilon_{ij}, \tag{4} \\
\beta_{0j} &= \gamma_{00} + \gamma_{01}Z_{1j} + \upsilon_{0j}, \tag{4.1} \\
\beta_{1j} &= \gamma_{10} + \gamma_{11}Z_{1j} + \upsilon_{1j}, \tag{4.2} \\
\beta_{2j} &= \gamma_{20} + \gamma_{21}Z_{1j} + \upsilon_{2j}, \tag{4.3} \\
\beta_{3j} &= \gamma_{30} + \gamma_{32}Z_{2j} + \upsilon_{3j}, \tag{4.4} \\
\beta_{4j} &= \gamma_{40} + \upsilon_{4j}, \tag{4.5} \\
\beta_{5j} &= \gamma_{50} + \upsilon_{5j}, \tag{4.6} \\
\beta_{6j} &= \gamma_{60} + \upsilon_{6j}, \tag{4.7} \\
\beta_{7j} &= \gamma_{70}, \tag{4.8}
\end{align}
\end{subequations}
where $y_{ij}$ is a continuous level 1 outcome variable for person $i$ in group $j$ and $Z_{1j}$ and $Z_{2j}$ are continuous level 2 variables. The random intercept $\beta_{0j}$ is determined by the grand mean $\gamma_{00}$, the group effect $\gamma_{01}Z_{1j}$ and the group-level random residuals $\upsilon_{0j}$. The regression coefficients $\beta_{1j}$, $\beta_{2j}$, and $\beta_{3j}$ for the continuous variables $X_{1ij}$, $X_{2ij}$, and $X_{3ij}$ depend on the the intercepts $\gamma_{10}$, $\gamma_{20}$, and $\gamma_{30}$, the cross-level interactions $\gamma_{11}Z_{1j}$, $\gamma_{21}Z_{1j}$, and $\gamma_{32}Z_{2j}$, and the random slopes $\upsilon_{1j}$, $\upsilon_{2j}$, and $\upsilon_{3j}$. The regression coefficients $\beta_{4j}$, $\beta_{5j}$ and $\beta_{6j}$ are determined by the intercepts $\gamma_{40}$, $\gamma_{50}$ and $\gamma_{60}$ and the random slopes $\upsilon_{4j}$, $\upsilon_{5j}$ and $\upsilon_{6j}$. The regression coefficient $\beta_{7j}$ is determined by the intercept $\gamma_{70}$. The residuals and random slopes $\upsilon_{0j}$, $\upsilon_{1j}$, $\upsilon_{2j}$, $\upsilon_{3j}$, $\upsilon_{4j}$, $\upsilon_{5j}$, and $\upsilon_{6j}$ and $\epsilon_{ij}$ follow a zero-mean normal distribution. The variance of $\upsilon_{0j}$, the group-level random residuals, were scaled such that the specified ICC value was obtained. $\upsilon_{1j}$, $\upsilon_{2j}$, $\upsilon_{3j}$, $\upsilon_{4j}$, $\upsilon_{5j}$, and $\upsilon_{6j}$ all have a variance of 1. $\epsilon_{ij}$ had a variance of 25. $X_1$, $X_2$, $X_3$, $X_4$, $X_5$, $X_6$ and $X_7$ are multivariate normally distributed: $\mathbf{X}_{ij} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, with $\boldsymbol{\mu} = (0, 0, 0, 0, 0, 0, 0)$ and $\boldsymbol{\Sigma} = \text{diag}(6.25, 9, 4, 11.56, 4, 2.5, 19.36)$ with no co-variances. The level-2 variables $Z_1$ and $Z_2$ are also multivariate normally distributed:  $\mathbf{Z}_{j} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, with $\boldsymbol{\mu} = (0, 0)$ and $(\boldsymbol{\Sigma}) = \text{diag}(1, 2.56)$ with no co-variances. The group-level effects ($\gamma_{01}$ and $\gamma{02}$) were set to .5, the cross-level interactions ($\gamma_{11}$, $\gamma_{21}$, and $\gamma_{32}$) were set to .35, and the overall intercept ($\gamma_{00}$) was set to 10. The within-group effect sizes ($\gamma_{10}$, $\gamma_{20}$, $\gamma_{30}$, $\gamma_{40}$, $\gamma_{50}$, $\gamma_{60}$, and $\gamma_{70}$) were varied in the simulations.

\subsubsection{Simulation design}
\begin{wraptable}{r}{8cm}
\centering
\caption{Simulation design}
\label{tab:simulationparameters}
\begin{tabular}{l|l}
\textbf{Parameter}                                  & \textbf{Values} \\ \hline
Number of clusters (j)                              & 30, 50          \\
Within-cluster sample size (n\textsubscript{j})     & 5, 15, 35, 50   \\
Intraclass Correlation (ICC)                        & 0, .05, .3, .5  \\
Within-group effect size ($\gamma$)                 & .2, .5, .8
\end{tabular}
\end{wraptable}

Table \ref{tab:simulationparameters} shows the factors that will be varied in the simulation study. All these values are realistic in practice and/or previously proposed \citep{gulliford1999, murray2003, hox2017, grund2018, enders2018a, enders2020}. The ICC can be interpreted as the expected correlation between two randomly sampled individuals from the same group or the proportion of the total variance at level-2 \citep{gulliford2005, shieh2012, hox2011}. For each combination of varying parameters, 6 data sets will be simulated for each scenario to reduce computational time. 576 data sets will be simulated in total. 4 different models will be compared: a single level BART, single level BART with group-dummy's, a multilevel BART model incorporating a random intercept \citep{chen2020, wagner2020, tan2016, wundervald2022}, and a multilevel BART model combined with Stan to model the random parts of the models \citep{dorie2022}. The first three models will be performed in R \citep{rcoreteam2023} with the package \textit{dbarts} \citep{dorie2023} and the last will performed with the R-package \textit{stan4bart} \citep{dorie2023a}. The default arguments from the function \textit{rbart\_vi} will be used for all models, as well as the default priors.

\subsubsection{Evaluation}
In this research report, the preliminary results will pertain to the evaluation of different BART models in terms of the relative bias (the difference between the average estimate and the true value) and Mean Squared Error (MSE) of the estimates, which will be calculated as follows \citep{morris2019}:
\begin{subequations}
\label{eq:evaluations}
\begin{align}
Bias &= \frac{1}{n_{\text{sim}}} \sum_{t=1}^{n_{\text{sim}}} (\hat{\theta}_t - \theta), \tag{5} \\
MSE &= \frac{1}{n_{\text{sim}}} \sum_{t=1}^{n_{\text{sim}}} (\hat{\theta}_t - \theta)^{2}, \tag{6} \\
\end{align}
\end{subequations}
where $\hat{\theta}_t$ is the estimated parameter in simulation \textit{t}, $\theta$ is the true value, and $n_{\text{sim}}$ is the number of simulated data sets.

\section{Results}
\graphicspath{{./graphs/}}

The results of this research report are shown in figure \ref{fig:biasplots} and \ref{fig:mseplots}. Figure \ref{fig:biasplots} shows the average relative bias for all models, simulated data sets, every \textit{ICC} value, and within-group effect size. On the x-axis we can see the different simulated data sets with their names specifying the total sample size, number of groups and group sizes. Figure \ref{fig:mseplots} shows the average Mean Squared Error (MSE) for all models, data sets, \textit{ICC} values, and within-group effect sizes.

In figure \ref{fig:biasplots} we can see that when there is no multilevel structure in the data set, $ICC = 0$, the models perform similarly in terms of relative bias: overall, the bias is around zero. We can see a slight increase in relative bias  when the total sample size is small for all within-group effect sizes.
When we increase the amount of multilevel structure in the data set, $ICC > 0$, we can see a divide in the performance of the models. Even with a little multilevel structure, $ICC = .05$, the single level BART model (bart) increases in relative bias. The single level BART model shows higher levels of relative bias when the total sample size is small. However, when the total sample size is large, \textit{2500}, the single level BART model performs slightly better than the single level BART model including a group-dummy for all within-group effect sizes. The same pattern is observed for the other models: when $ICC = .05$, the relative bias decreases when the total sample size increases.
This pattern is further continued when increasing the \textit{ICC}: the relative bias decreases when the total sample size increases for both $ICC = .3$ and $ICC = .5$. However, when considering the stan4bart model, which models the random parts of the model in Stan and the fixed parts in BART, the relative bias stays considerably constant when increasing the \textit{ICC}: the relative bias is higher when the total sample size is small, but it does not seem to significantly increase when introducing more multilevel structure or increasing the within-group effect size.
This cannot be said for the bart and gbart models, which show a slight increase in relative bias when increasing the \textit{ICC}, especially in the lower total sample sizes. The rbart model, which incorporates a random intercept, shows a similar pattern as the stan4bart model, but with an overall higher relative bias.

\begin{figure}[H]
\caption{Bias of the estimates for all simulated data sets, all ICC values, and differing within group effect sizes using four models. bart = single level BART model, gbart = single level BART model with group-dummys, rbart = multilevel BART model incorporating a random intercept, stan4bart = multilevel BART model combined with Stan to model the random parts of the models. Data set names show the total sample size, number of groups, and group size. Bias is based on 6 replicates for every data set.}
\centering
\label{fig:biasplots}
\includegraphics[width=\textwidth]{biasplots4.pdf}
\end{figure}

Figure \ref{fig:mseplots} shows a similar pattern as figure \ref{fig:biasplots}. When there is no multilevel structure in data sets, $ICC = 0$ the models perform well and almost exactly the same. When introducing multilevel structure, we can start to see a divide in the performance of the models. When $ICC = .05$ the models bart, gbart and rbart perform similarly, with gbart outperforming the other two models when the group sizes are 35 or larger. However, stan4bart consistently outperforms the other three models in terms of MSE. Introducing more multilevel structure to the data, $ICC = .3$, divides the performance of the models further: in small data sets bart now has the highest MSE with gbart performing only a little better. rbart performs better than bart and gbart, but when the data sets increase in size, it performs similar to bart. Again, gbart outperforms bart and rbart when the groups sizes are 35 or larger. stan4bart is still the hast the overall lowest MSE. Increasing the \textit{ICC} further to .5 exaggerates these patterns. Over all \textit{ICC} values, increasing the within group effect size increases the MSE of the models.

\begin{figure}[H]
\caption{Mean Squared Error (MSE) of the estimates for all simulated data sets, all ICC values, and differing within group effect sizes using four models. bart = single level BART model, gbart = single level BART model with group-dummys, rbart = multilevel BART model incorporating a random intercept, stan4bart = multilevel BART model combined with Stan to model the random parts of the models. Data set names show the total sample size, number of groups, and group size. MSE is based on 6 replicates for every data set.}
\centering
\label{fig:mseplots}
\includegraphics[width=\textwidth]{mseplots4.pdf}
\end{figure}

\newpage
\section{Discussion}

In this research report I have investigated the performance of different BART models in terms of relative bias and MSE of the estimates. I considered four different models: a single-level BART model (bart), a single-level BART model including a group-dummy (gbart), a multilevel BART model including a random intercept (rbart), and a multilevel BART model combining Stan and BART (stan4bart). \citet{chen2020, tan2016} and \citet{wundervald2022} found that a BART model including a random intercept performed better than a single-level BART model, which is partly in agreement with my results: the MSE for the rbart showed more consistency, but when the total sample sizes were large, the model did not perform better than the bart and gbart models. The results indicate that the \textit{stan4bart} model performs best out of the four models: it shows to lowest relative bias as well as the lowest MSE. These results are agreement with \citet{dorie2022}, who found that the \textit{stan4bart} algorithm performed better in terms of coverage of the population values and RMSE compared to single-level BART models, BART models including a random intercept, Bayesian Causal Forests (BCF), and parametric multilevel models.

\begin{wraptable}{r}{8cm}
\centering
\caption{Simulation design thesis}
\label{tab:simulationparameters2}
\begin{tabular}{l|l}
\textbf{Parameter}                                  & \textbf{Values} \\ \hline
Number of clusters (j)                              & 30, 50          \\
Within-cluster sample size (n\textsubscript{j})     & 5, 15, 35, 50   \\
Intraclass Correlation (ICC)                        & 0, .05, .3, .5  \\
Missing data mechanism                              & MAR, MCAR       \\
Amount of missingness                               & 0\%, 25\%, 50\% \\
Within-group effect size ($\gamma$)                 & .2, .5, .8
\end{tabular}
\end{wraptable}

Building on these results, I will implement the \textit{stan4bart} as an imputation method within the package \textit{MICE} \citep{buuren2011} model in my thesis. I will compare the performance of the \textit{stan4bart} model to other imputation methods: \textit{2l.pmm, 2l.lmer, 2l.pan, 2l.jomo, rf} and single-level \textit{pmm} and complete case analysis in the R-package MICE \citep{buuren2011}. The performance of the models will be evaluated in terms of relative bias, modeled variance, and the 95\% confidence interval coverage of the estimates \citep{oberman2023}. The simulation design will be extended to include more parameters: the missing daa mechanism and amount of missingness. The simulation design is shown in table \ref{tab:simulationparameters2}. For each combination of paramteres, a 1000 replicated data sets will be generated.

\newpage
\bibliography{thesis}
\bibliographystyle{apalike}

\end{document}

